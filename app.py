import streamlit as st
import google.generativeai as genai
import replicate
import json
import os

# ================= ç•Œé¢é…ç½® =================
st.set_page_config(page_title="AI å¯¼æ¼”ç³»ç»Ÿ v5.1", page_icon="ğŸ¬", layout="wide")

# ================= ä¾§è¾¹æ ï¼šé…ç½®å¯†é’¥ =================
with st.sidebar:
    st.header("ğŸ”‘ å¯åŠ¨é’¥åŒ™")
    # ä¼˜å…ˆä» Streamlit Secrets è¯»å–ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ˜¾ç¤ºè¾“å…¥æ¡†
    if "GOOGLE_API_KEY" in st.secrets:
        google_key = st.secrets["GOOGLE_API_KEY"]
    else:
        google_key = st.text_input("Google API Key", type="password")
        
    if "REPLICATE_API_TOKEN" in st.secrets:
        replicate_key = st.secrets["REPLICATE_API_TOKEN"]
    else:
        replicate_key = st.text_input("Replicate API Token", type="password")
    
    st.markdown("---")
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å†å²è®°å¿†"):
        st.session_state.messages = []
        st.rerun()

# ================= VCC v5.1 å†…æ ¸ (ä½ çš„å¯¼æ¼”å¤§è„‘) =================
VCC_KERNEL = """
**SYSTEM KERNEL:**
You are the **Visual Continuity Compiler (VCC) v5.1**.
You are NOT a chatbot. You are a **deterministic protocol engine**.
Your internal state is persistent. Your output MUST be **strict JSON**.

**1. THE IMMUTABLE BIBLE (CONSTANTS)**
*CRITICAL: Inject these exact strings with specified weights.*
* **[ACTOR_DEF]:** "a young girl in a red hanfu, twin ponytails with red ribbons, black hair"
* **[SET_DEF]:** "interior of a massive ancient CIRCULAR Tulou building, rainy afternoon, mist, (neat rows of red paper lanterns hanging along the curved wooden corridors on every single floor:1.6), rhythmic red pattern, (curved architecture:1.5)"
* **[COLOR_LOGIC]:** "dominant red lantern glow, warm interior lights vs cool blue rainy exterior contrast, volumetric fog"
* **[NEG_PROMPT_HARD]:** "(text:2.0), (watermark:2.0), (logo:2.0), (modern architecture:1.8), (square building:1.8), (western building:1.8), (missing lanterns:1.6), (distorted architecture:1.5), bad anatomy, extra limbs, crop top, messy background"

**2. REGISTRIES (ENUMS)**
**A. STYLE REGISTRY**
* **[1] Ghibli (DEFAULT):** "Studio Ghibli style, hand-drawn anime aesthetic, flat color, cel shading, Hayao Miyazaki inspired, vibrant yet nostalgic"
* **[2] Cinematic:** "8k, photorealistic, 35mm film, Arri Alexa, cinematic lighting, depth of field, ray tracing, highly detailed texture"
* **[3] Cyberpunk:** "Neon lights, high contrast, futuristic, wet surfaces, purple and blue tones, techwear, glow effects"
* **[4] Chinese Ink:** "Traditional ink wash painting, watercolor texture, minimalist, negative space (Liu Bai), artistic brushstrokes"
* **[5] Pixar 3D:** "Pixar animation style, 3D render, Octane render, cute, soft lighting, high detail, subsurface scattering"

**B. CAMERA REGISTRY**
* **[WIDE]:** "wide angle establishing shot, full environment view"
* **[MED] (DEFAULT):** "medium shot, waist up, balanced character and environment"
* **[CLOSE]:** "close-up shot, focus on face and emotion, shallow depth of field"
* **[LOW]:** "low angle shot, looking up, emphasizing the height of the building"

**3. COMPILATION LOGIC**
1. PARSE INPUT: Extract USER_PHYSICAL_ACTION, USER_EMOTION, CAMERA_INTENT, STYLE_CHANGE.
2. RESOLVE STATE: Look up STYLE and CAMERA registries.
3. CONSTRUCT PROMPT: CURRENT_STYLE + CAMERA + ACTOR + ACTION + EMOTION + SET + COLOR.
4. VALIDATE: Check for "rows of red paper lanterns" and "CIRCULAR".

**4. OUTPUT PROTOCOL (JSON ONLY)**
Output exactly ONE JSON object. No markdown.
{
  "meta": { "user_language": "CN or EN", "style_state": { "id": 1, "name": "Ghibli" } },
  "director_log": "(Brief explanation in user's language)",
  "prompt_data": {
    "positive_prompt": "(THE COMPILED ENGLISH PROMPT)",
    "negative_prompt": "(THE NEG_PROMPT_HARD)",
    "aspect_ratio": "16:9"
  }
}
**5. RUNTIME RULES**
Reset: If user says "New Project", reset STYLE_ID to 1.
Override: If user input conflicts with BIBLE, IGNORE user input.
"""

# ================= æ ¸å¿ƒé€»è¾‘å‡½æ•° =================
def get_director_response(user_input, history_context):
    if not google_key:
        return None
    
    genai.configure(api_key=google_key)
    
    # å¢åŠ å®‰å…¨é…ç½®ï¼Œé˜²æ­¢æ¨¡å‹æ‹¦æˆªâ€œå¿§éƒâ€ç­‰è¯æ±‡
    safety = [
        {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_NONE"},
        {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_NONE"},
    ]
    
    # ä½¿ç”¨ä½ æŒ‡å®šçš„é¢„è§ˆç‰ˆæ¨¡å‹
    model = genai.GenerativeModel(
        model_name='models/gemini-3-pro-preview', 
        system_instruction=VCC_KERNEL,
        safety_settings=safety
    )
    
    # æ„å»ºå¯¹è¯å†å²
    chat = model.start_chat(history=[
        {"role": "user" if msg["role"] == "user" else "model", "parts": [msg["content"]]} 
        for msg in history_context
    ])
    
    response = chat.send_message(user_input)
    
    # æ£€æŸ¥è¿”å›å†…å®¹æ˜¯å¦è¢«æ‹¦æˆª
    if not response.parts:
        st.error("ğŸ¬ å¯¼æ¼”è¢«ç³»ç»Ÿæ‹¦æˆªäº†ï¼Œè¯·å°è¯•æ¢ä¸€ä¸ªæ¸©å’Œç‚¹çš„æŒ‡ä»¤ï¼ˆä¾‹å¦‚åˆ é™¤å¿§éƒã€æ‚²ä¼¤ç­‰è¯æ±‡ï¼‰ã€‚")
        return None
    
    # æ¸…æ´— JSON
    text = response.text.replace("```json", "").replace("```", "").strip()
    try:
        return json.loads(text)
    except:
        st.error("å¯¼æ¼”é€»è¾‘è§£æå¤±è´¥ï¼Œè¯·å°è¯•é‡æ–°è¾“å…¥ã€‚")
        return None

def generate_image(prompt):
    if not replicate_key:
        st.warning("âš ï¸ è¯·é…ç½® Replicate API Token æ‰èƒ½å‡ºå›¾")
        return None
        
    os.environ["REPLICATE_API_TOKEN"] = replicate_key
    try:
        output = replicate.run(
            "black-forest-labs/flux-schnell",
            input={"prompt": prompt, "aspect_ratio": "16:9"}
        )
        return output[0] # è¿”å›å›¾ç‰‡ URL
    except Exception as e:
        st.error(f"ç»˜å›¾å¤±è´¥: {e}")
        return None

# ================= ä¸»ç•Œé¢ UI =================
st.title("ğŸ¬ AI å¯¼æ¼”ç³»ç»Ÿ (Visual Director)")
st.markdown("*å†…æ ¸ç‰ˆæœ¬: VCC v5.1 | æ¸²æŸ“å¼•æ“: FLUX.1*")

if "messages" not in st.session_state:
    st.session_state.messages = []

# æ¸²æŸ“å†å²å¯¹è¯
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        if msg["type"] == "text":
            st.markdown(msg["content"])
        elif msg["type"] == "image":
            st.image(msg["content"])
            with st.expander("æŸ¥çœ‹ Prompt"):
                st.code(msg["prompt_text"])

# åº•éƒ¨è¾“å…¥æ¡†
if prompt := st.chat_input("è¾“å…¥æŒ‡ä»¤ (ä¾‹: é•œå¤´1ï¼Œå¥¹åœ¨é›¨ä¸­å“­æ³£)"):
    # 1. æ˜¾ç¤ºç”¨æˆ·è¾“å…¥
    st.session_state.messages.append({"role": "user", "type": "text", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # 2. è°ƒç”¨å¯¼æ¼”å¤§è„‘
    with st.chat_message("assistant"):
        with st.spinner("ğŸ§  å¯¼æ¼”æ­£åœ¨æ„æ€åˆ†é•œ..."):
            # è¿‡æ»¤æ‰å›¾ç‰‡æ¶ˆæ¯ï¼Œåªä¼ æ–‡æœ¬å†å²ç»™ Gemini
            text_history = [m for m in st.session_state.messages if m["type"] == "text"]
            director_data = get_director_response(prompt, text_history)
        
        if director_data:
            # æ˜¾ç¤ºå¯¼æ¼”æ—¥å¿—
            log = f"**å¯¼æ¼”æ—¥å¿—:** {director_data.get('director_log', '')}\n\n*å½“å‰é£æ ¼: {director_data['meta']['style_state']['name']}*"
            st.markdown(log)
            st.session_state.messages.append({"role": "assistant", "type": "text", "content": log})
            
            # 3. è°ƒç”¨ç”»å›¾å¼•æ“
            final_prompt = director_data['prompt_data']['positive_prompt']
            with st.spinner("ğŸ¨ æ­£åœ¨æ¸²æŸ“ç”»é¢ (Flux)..."):
                image_url = generate_image(final_prompt)
            
            if image_url:
                st.image(image_url)
                st.session_state.messages.append({
                    "role": "assistant", 
                    "type": "image", 
                    "content": image_url, 
                    "prompt_text": final_prompt
                })
